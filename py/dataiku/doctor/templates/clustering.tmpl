{% extends "core.tmpl" %}

{% block head %}
## Clustering
## This notebook will reproduce all the steps for clustering the dataset {{ dataset }}.
{% endblock %}

{% block profiling_copy %}
## Let's copy our dataset to keep it for eventual profiling at the end.

# train dataset will be the one on which we will apply ml technics
train = ml_dataset.copy()
{% endblock %}


{% block preprocessing %}
{{ super() }}
## Removing outliers

# Outliers detection not required, skipping...

{% if outliers.method == "none" %}
# Outliers detection not required, skipping...
{% else %}
from dataiku.doctor.preprocessing.dataframe_preprocessing import detect_outliers

outliers = detect_outliers(train, {{ reduce.kept_variance if reduce.kept_variance > 0 else 0.9 }}, {{ outliers.min_n }}, {{ outliers.min_cum_ratio }})
train = train[~outliers]
{% endif %}

{% endblock %}


{% block modeling %}



{% if algorithm == 'KMEANS' %}
from sklearn.cluster import KMeans
clustering_model = KMeans(n_clusters={{ modeling_params.k }})
{% elif algorithm == 'MiniBatchKMeans' %}
from sklearn.cluster import MiniBatchKMeans
cltr = MiniBatchKMeans(n_clusters={{ modeling_params.k }})
{% elif algorithm == 'SPECTRAL' %}
from sklearn.cluster import SpectralClustering
cltr = SpectralClustering(n_clusters={{ modeling_params.k }})
{% elif algorithm == 'WARD' %}
from sklearn.cluster import Ward
clustering_model = Ward(n_clusters={{ modeling_params.k }})
{% elif algorithm == 'DBSCAN' %}
from sklearn.cluster import DBSCAN
clustering_model = DBSCAN(eps=modeling_params.epsilon)
{% endif %}

## We can finally cluster our dataset!

%time clusters = clustering_model.fit_predict(train)

{% endblock %}


{% block evaluation %}

{% if is_kmean_like %}
## Inertia

print clustering_model.inertia_
{% endif %}


{% if modeling_params.k > 1 %}

## Silhouette

from sklearn.metrics import silhouette_score
silhouette = silhouette_score(train.values, clusters, metric='euclidean', sample_size=2000)
print "Silhouette score :", silhouette
{% endif %}



## Join our original dataset with the cluster labels we found.

final = train.join(pd.Series(clusters, index=train.index, name='cluster'))
final['cluster'] = final['cluster'].map(lambda cluster_id: 'cluster' + str(cluster_id))

## Compute the cluster sizes
size = pd.DataFrame({'size': final['cluster'].value_counts()})
size.head()


## Draw a nice scatter plot

axis_x = train.columns[0]   # change me
axis_y = train.columns[1]  # change me

from ggplot import ggplot, aes, geom_point
ggplot(aes(axis_x, axis_y, colour='cluster'), final) + geom_point()




{% endblock %}
